# Multilayer Perceptron

## Project

### Requirements

These projects is built from scratch with Python, with Numpy, Pandas, Matplotlib..  

A `venv.sh` and `requirements.txt` to help you set up a virtual environment and install the necessary packages:

```bash
git clone https://github.com/swangarch/multilayer_perceptron
cd multilayer_perceptron

bash venv.sh
source venv/bin/activate
```

### Multilayer perceptron

Multilayer perceptron is implemented as an class in Python,  
capable of solving both regression and classification tasks.  
The design goal is to provide a reusable and modular implementation.

### Features

- Algorithm from **scratch** → implemented without high-level ML libraries, only using NumPy for matrix operations.

- Training methods → supports Stochastic Gradient Descent (SGD) and mini-batch training.

- Visualization → includes real-time animations to track the learning process.

- Configurable architecture → adjustable network shape, activation functions, initialization methods, max iterations, learning rate, batch size, etc.

- Data handling → built-in data loading and preprocessing pipeline.

- Support multiple activation functions: ReLU, Sigmoid, Softmax, Leaky ReLU

- Support multiple loss functions: 
  - Categorical Cross Entropy (CCE) with automatic one-hot encoding for categorical labels
  - Binary Cross Entropy (BCE)
  - Mean Squared Error (MSE)

- Support Fine-tuning: continue training from saved weights for further optimization
  

### Programs 

#### Configuration

We recommand you to use --help option before the first usage.

An json config file to pass network structure and training configuration is required.

```json
      {
          "shape": [784, 64, 32, 10],  
          // Network shape, number of neurons in each layer

          "activation_funcs": ["relu", "relu", "softmax"],  
          // Activation functions for each layer

          "weights_init": ["he", "he", "xavier"],  
          // Layer weights initialization methods, e.g. he / xavier / zero

          "loss": "CrossEntropy",  
          // Loss function, supporting MeanSquareError and CrossEntropy

          "max_epoch": 1000,  
          // Maximum number of training epochs

          "learning_rate": 0.01,  
          // Learning rate for gradient descent

          "batch_size": 500,  
          // Batch size for mini-batch training， 0 for SGD

          "classification": true,  
          // Whether to calculate accuracy (true for classification tasks)

          "animation": "none",  
          // To show training animation, recommended only for 1D data

          "train_ratio": 0.8,  
          // Ratio to split dataset into training and validation

          "threshold": false,  
          // Threshold for early stopping, false means disabled

          "index": false  
          // Whether the input CSV file contains an index column
      }
```
#### Train and predict

```
      Usage:

        python  mlp.py  <--options> <config.json> <data.csv> [params.json]

      Options:

        -s:  Split dataset into (train.csv) and (test.csv), no params required

        -t:  Train dataset, if params.json is provided, it performs fine tuning.

        -p:  Test on a test dataset, if params.json is provided, it performs prediction based on trained model.

        -help:  Show help messages.

        -More features to come.

      Note:

        There is built in dataset generated by script, use --gen-data1d to replace dataset
```

The program will:

- Defines the network structure (layers, activation functions).  

- Trains using backpropagation and gradient descent.  

- Visualize loss and prediction:

- By default, regression task will visualize the loss, and the data  distribution for first dimension.

- In addition, classification task will add an accuracy plot.


### Demo
The example graph on both classification and regression.

![Demo](visualization/3_nn/demo.png)


### Datasets

Datasets are provided in this repository, one example corresponding to breast cancer diagnosis classification.
It contains 32 columns, where the diagnosis column is the label to predict. The label can take two values:

M → Malignant (cancerous)

B → Benign (non-cancerous)

The remaining 30 numerical features describe various characteristics of a cell nucleus extracted with fine-needle aspiration (FNA), such as radius, texture, perimeter, area, and smoothness. These measurements are widely used for early breast cancer detection.

#### To split data into train.csv and test.csv:

It is split into two parts: one for training and validation one for testing.

```python
python mlp.py -s config/data-softmax.json data/data.csv
```

The objective is to build a classification algorithm that predicts whether a tumor is malignant or benign based on the given features.

#### To train
```python
python mlp.py -t config/data-softmax.json data/train.csv (params.json)
```
Not if params.json is provided, the program will perform fine tuning.

#### To test
```python
python mlp.py -p config/data-softmax.json data/test.csv params.json
```

This dataset support both BCE and CCE, if data-sigmoid.json is used, it will perform BCE other wise CCE

The program will perform prediction, an accuracy will be calculated and prediction will be saved in predictions.json.

#### All datasets and configurations provided
- 0-1.csv _____ 0-1.json => 0 to 1 hand wirte digits recognition with BCE 
- 0-9.csv _____ 0-9.json => 0 to 9 hand wirte digits recognition with CCE
- --gen-data1d _____ regre-relu.json => 1d random generated data with RELU activation and MSE loss
- --gen-data1d _____ regre-sigmoid.json => 1d random generated data with Sigmoid activation and MSE loss

### Class 

Example: Train and Test a Neural Network

Below is a simple example of using the `NN` class to train a regression model.

```python
from Neural_network import NN, get_activation_funcs_by_name
from Data_process import load, preprocess_data, conf_parser, generate_data_rand

def main():
    try:
        # Define network structure: input → 64 → 32 → output
        net_shape = (1, 64, 32, 1)
        activation_funcs = (relu, relu, None)  # last layer is linear (regression)
        weights_init = ("he", "he", "he")  # Define the weights initialization methods

        # Initialize neural network
        nn = NN(net_shape, activation_funcs)

        # Generate training data
        inputs, truths = generate_data_rand(142, 500, 0.02)
        test_inputs, test_truths = generate_data_rand(123, 50, 0.02)
        nn.train(inputs, truths, 10000, 0.005, batch_size=20, animation="plot")

        # Test with new data
        nn.test(inputs, truths, test_inputs, test_truths)

        # Save graphs
        nn.save_plots()

    except Exception as e:
        print("Error:", e)


if __name__ == "__main__":
    main()
```

### Generalization

The figure shows that neural networks are capable of learning non-linear patterns from different datasets and can generalize beyond the training data, making them applicable to a wide range of tasks.

![Regression demo](visualization/3_nn/prediction.jpg)

---

## License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
